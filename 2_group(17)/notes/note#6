## 앙상블 학습

-집단 지성을 모티브로 한 **여러 개의 예측기를 이용**해 더 좋은 예측을 도출하는 학습 방법이다.

- 앙상블: 여러 개의 예측기로 이루어진 **그룹**

**투표 기반 분류기**

직접투표: 예측기들의 예측값을 다수결 투표로 결정

간접투표: 예측기들의 예측한 확률값들의 평균값으로 예측값 결정(직접 투표에 비해 성능이 더 좋음) → 모든 분류기에 다 예측값이 있어야함 → SVC 함수 파라미터 probability를 True로 설정

큰 수의 법칙: 반복 시행의 횟수, 표본이 커질수록 일정한 수준으로 수렴 → 정확한 예측 가능

**배깅과 페이스팅**

여러 분류기가 학습 알고리즘이 같은 상황에서 훈련 세트를 서브 세트로 나누어 각각 학습시키는 방법 

- 배깅: 중복 허용 샘플링 방식
- 페이스팅: 중복 미허용 샘플링 방식

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7d632a17-141f-4749-9b96-724089e23adc/3260b3e8-3a43-4a2f-9465-42bcaa267a00/Untitled.png)

개별 예측기의 결과를 종합 → 최종 예측값 지정

- 분류 모델 : 최빈 값 선택
- 회귀 모델 : 평균 값 선택

앙상블 기법의 배깅 및 페이스팅 사용 시 편향과 분산 

- 개별 예측기와 비교해 편향은 비슷, 분산은 줄어듦(일반화하기 쉽다) → 과대적합 위험성이 줄어든다.

개별 예측기 배깅 및 페이스팅 사용 시 편향

- 편향이 커지고, 과소적합 위험성 커짐

BaggingClassifier 함수 호출 시 bootstrap 파라미터가 배깅/페이스팅을 결정(페이스팅 → False)

oob 평가 : 선택되지 않은 샘플(out-of-bag 샘플)들을 이용해서 앙상블 학습에 사용된 개별 예측기의 성능을 평가

**랜덤 패치, 랜덤 서브스페이스**

랜덤 패치: 훈련 샘풀과 훈련 특성 모두 중복 허용,임의의 샘플 수와 특성 수만큼 샘플링해서 학습하는 기법

랜덤 서브스페이스: 전체 훈련 세트를 학습 대상으로 삼지만 훈련특성은 임의의 특성 수만큼 샘플링해서 학습하는 기법

## 랜덤 포레스트

결정 트리를 앙상블 방법으로 구현한 것.

배깅/페이스팅 방법을 적용한 결정트리의 앙상블을 최적화한 모델

트리의 노드 분할 시 최선의 특성을 찾는 대신 무작위성 높임 → 편향을 손해보는 대신 분산을 낮춤(일반화)

랜덤 포레스트의 노드 분할 방식 - **특성을 무작위로 선택 이후 최적값 선택(특성 임계값)**

엑스트라 트리의 노드 분할 방식 - **특성 및 임계값 모두 무작위 선택**, 랜덤 포레스트보다 속도가 빠르고, 편향은 늘고 분산은 줄어듦

**특성 중요도** - 해당 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 측정 (불순도를 줄이면 중요도가 커짐)

**부스팅** - 성능이 약한 학습기 여러 개를 연결해 강한 성능의 학습기로 만드는 앙상블 기법(순차적 → 확장성이 떨어짐)

- 에이다부스트 - 이전 예측기에서 과소적합한 훈련 샘플의 가중치를 더 높이는 방식으로 학습을 진행

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7d632a17-141f-4749-9b96-724089e23adc/4e4a2dd0-8eb0-49a9-87af-a07701e2d8e3/Untitled.png)

- 그레이디언트 부스팅 - 샘플의 가중치를 수정하는 대신 잔여 오차(예측값과 실제값 사이의 오차 )에 대해 새로운 예측기를 학습시킴
    
    → 잔여 오차를 줄이는데 “경사 하강법”사용하여 최적화함
    
    분류모델: GradientBoostingClassifier
    
    회귀모델: GradientBoostingRegressor
    
- 확률적 그레디언트 부스팅 - 각 결정트리 훈련에 사용할 훈련 샘플의 비율을 지정하여 학습(속도가 빠름, 편향 높아짐, 분산 낮아짐)

스태킹 - 예측을 취합하는 모델을 훈련하는 것

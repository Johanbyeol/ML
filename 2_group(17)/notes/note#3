## 모델 훈련

- **공식을 통해 최적값(파라미터) 계산**
    
    선형 회귀 모델의 MSE 비용 함수에서 MSE를 최소화하는 θ값을 찾아야 함
    
    ### 1. 정규방정식
    
    비용 함수를 최소화하는 θ값을 찾기 위한 해석적 방법  
    
    → 모델의 복잡도가 훈련 세트의 샘플 수와 특성 수에 선형적으로 증
    
    ### 2. SVD(Singular Value Decomposion) - 특이값 분해
    
    ### 계산 복잡도
    
    정규방정식 → O(n^2.4) ~ O(n^3) 
    
    SVD → LinearRegression 클래스의 SVD는 O(n^2)
    
    따라서, 특성 수가 많은 경우 메모리 관리 및 시간복잡도 때문에 비효율적.
    
- **경사 하강법(GD)**
    
    경사 하강법 : 여러 종류의 문제에서 최적의 값을 찾는 알고리즘
    
    비용 함수 최소화를 위해 → 반복적으로 파라미터 조정(스텝의 크기가 중요)
    
    원리 : 무작위로 θ(임의의 값)에서 시작 → 비용 함수가 감소하는 방향으로 진행(최솟값=기울기가 0이 될 때까지)
    
    경사 하강법의 중요 파라미터 - 학습률 하이퍼파라미터(스텝의 크기)
    
    학습률이 작을 때 → 시간이 오래 걸림(지역 최솟값에 빠진다 )
    
    학습률이 클 때 → 발산
    
    따라서, 학습률을 잘 조정해야 최적의 값으로 잘 수렴할 수 있다 
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/7d632a17-141f-4749-9b96-724089e23adc/b99b307d-c49e-4cb7-a106-9dd18fbbb0ce/Untitled.png)
    
    단점 : 최솟값 수렴이 어려움. ex) 특이한 지형, 지역 최솟값, 평탄한 지역 (계산이 오래 걸림)
    
    ### 1.  배치 경사 하강법
    
    전체 훈련 데이터로 손실함수를 계산하여 그레이디언트 계산→한번만 수행
    
    편 도함수 - 모델 파라미터 θj가 조금 변경될 때 비용 함수가 얼마나 바뀌었는지 계산
    
    즉, 미분값의 변화를 관찰
    
    에폭(Epoch) = 가중치 최적화를 위해 데이터의 MSE 비용 함수 결과 값의 평균을 구하는 과정 1번 →배치 경사 하강법에서 에폭은 전체 데이터 셋을 말한다 
    
    배치 경사 하강법 특징
    
    1. 업데이트 횟수가 적음 ( 1 Epoch = 1 update )
    2. 전체 데이터를 한번에 처리해서, 메모리가 많이 필요
    3. 전체 데이터의 경사를 계산 → 수렴이 안정적
    
    ### 2.  확률적 경사 하강법
    
    배치 경사 하강법과 달리 전체 데이터가 아닌 **한 개의 샘플 데이터를 랜덤으로 선택해서 그레디언트 계산**
    
    특징: 적은 데이터로 학습, 속도가 빠름 → 확률적이므로 전역 최솟값을 찾는것이 불안정함
    
    학습 스케줄 - 학습률을 점차 줄여서 전역 최솟값에 도달하도록 유도
    
    ### 확률적 경사 하강법 코드
    
    ```
    n_epochs = 50
    t0, t1 = 5, 50
    
    def learning_schedule(t):
        return t0 / (t + t1)
    
    theta = np.random.randn(2,1)  
    
    #에폭(=50)만큼 반복문실행
    for epoch in range(n_epochs):
    		#
        for i in range(m):
    				#경사하강법 실행
            if epoch == 0 and i < 20:                  
                y_predict = X_new_b.dot(theta)          
                style = "b-" if i > 0 else "r--"        
                plt.plot(X_new, y_predict, style)       
            random_index = np.random.randint(m)
            xi = X_b[random_index:random_index+1]
            yi = y[random_index:random_index+1]
    				#기울기값 업데이트
            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
    		    #학습률을 현재 에폭과 반복 횟수에 따라 계산
            eta = learning_schedule(epoch * m + i)
    				#학습률과 기울기로 θ값 업데이트
            theta = theta - eta * gradients
    				#각 반복마다 업데이트하여 저장
            theta_path_sgd.append(theta)
    ```
    
    ### 3.  미니 배치 경사 하강법
    
    미니배치라 부르는 작은 샘플 세트로 그레디언트 계산(BGD와 SGD의 절충안임)
    
    SGD에 비해 상대적인 미니배치 경사 하강법의 장단점
    
    장점 : 
    
    epoc = 100, DataSet_size = 1000, minibatch_size = 50 일 때, 가중치 업데이트 횟수는?
    
    1. 미니배치의 개수 = DataSet_size / minibatch_size
    = 1000 / 50
    = 20
    
    즉, 한 에포크당 20번의 가중치 업데이트가 발생합니다.
    
    1. 총 가중치 업데이트 횟수 = 미니배치의 개수 × 에포크 수
    = 20 × 100
    = 2000
    
    결론적으로, 주어진 조건에서 가중치는 총 2000번 업데이트됩니다.
    
- **다항 회귀**
    
    비선형 데이터를 학습하기 위해 선형 모델을 사용하는 기법.
    
    각 특성을 거듭제곱 → 새로운 특성으로 추가 → 해당 특성을 포함하여 데이터셋에 선형 모델 훈련
    
    **학습곡선**
    
    **다항 회귀 모델의 차수에 따라** 훈련된 모델이 훈련 세트에 **과소 또는 과대 적합**할 수 있음
    
    학습곡선 : 훈련 세트와 검증 세트에 대한 모델 성능을 비교하는 그래프(과소/과대적합 판정 가능)
    
- **규제가 있는 선형 모델**
    
    모델을 규제 → 과대적합 감소
    
    1. 다항 회귀 모델의 다항식 차수 감소
    2. 자유도를 줄이며 데이터 과대적합 최소화
    3. 선형 회귀 모델에서는 **보통 모델의 가중치 제한**
    
    ### 릿지 회귀
    
    모델의 가중치가 가능한 작게 유지되도록 하는 것.
    
    → α값이 커질수록 가중치가 작아짐
    
    ![편향-분산의 의미 도식화](https://prod-files-secure.s3.us-west-2.amazonaws.com/7d632a17-141f-4749-9b96-724089e23adc/ec2dc409-3e63-43ea-8007-e830f97ad580/Untitled.png)
    
    편향-분산의 의미 도식화
    
    편향과 분산이 적을 수록 가중치가 작아짐
    
    ### 라쏘 회귀
    
    선형 회귀의 또 다른 규제된 버전
    
    가중치 값을 0으로 설정 → 자유도 X
    
    ### 엘라스틱 넷
    
    릿지 회귀와 라쏘 회귀를 절충한 모델
    
    [Γ](https://ko.wikipedia.org/wiki/%CE%93)(감마) 값을 사용하여 조절 ([Γ](https://ko.wikipedia.org/wiki/%CE%93) = 1  → 라쏘 회귀, [Γ](https://ko.wikipedia.org/wiki/%CE%93) = 0  → 릿지 회귀)
    
     
    
    ### 조기종료
    
    검증 에러가 최솟값(Best model)에 도달하면 훈련 중지
    
    ### 로지스틱 회귀
    
    독립변수의 선형 결합을 이용(**σ)**하여 사건 발생 가능성을 예측하는 통계 기법
    
    ### 훈련과 비용함수
    
    로지스틱 회귀 모델을 경사하강법을 이용하여 학습
    
    로지스틱 회귀 모델 비용함수 → 경사하강법 적용 → 그레이디언트 벡터를 만들면 배치 경사 하강법 적용
    
    ### 결정 경계
    
    꽃잎의 너비를 기반으로 특정 종을 감지하는 분류기 생성
    
    ### 소프트맥스 회귀
    
    로지스택 회귀 모델을 일반화 → 다중 클래스 분류를 지원
    
    == 다항 로지스틱 회귀
    
    해당 회귀의 비용함수 - 크로스 엔트로피 비용 함수 - 두 확률 분포의 차이를 구하기 위해 사용
